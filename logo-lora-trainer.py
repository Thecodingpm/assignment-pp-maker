#!/usr/bin/env python3
"""
Logo LoRA Trainer
Train a LoRA adapter specifically for logo generation
"""

import os
import json
import logging
from pathlib import Path
import requests
from PIL import Image
import torch
from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler
from peft import LoraConfig, get_peft_model, TaskType
import datasets

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LogoLoRATrainer:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model_id = "stabilityai/stable-diffusion-xl-base-1.0"
        self.lora_config = None
        self.model = None
        
    def prepare_logo_dataset(self, logo_images_dir, prompts_file):
        """Prepare dataset for logo training"""
        try:
            logger.info("üìÅ Preparing logo dataset...")
            
            # Load prompts
            with open(prompts_file, 'r') as f:
                prompts_data = json.load(f)
            
            # Prepare dataset
            dataset = []
            for i, (image_file, prompt_data) in enumerate(prompts_data.items()):
                image_path = os.path.join(logo_images_dir, image_file)
                
                if os.path.exists(image_path):
                    # Load and resize image
                    image = Image.open(image_path).convert('RGB')
                    image = image.resize((512, 512))
                    
                    # Create training example
                    example = {
                        "image": image,
                        "prompt": prompt_data["prompt"],
                        "style": prompt_data.get("style", "professional"),
                        "industry": prompt_data.get("industry", "general"),
                        "color": prompt_data.get("color", "modern")
                    }
                    dataset.append(example)
                    
            logger.info(f"‚úÖ Prepared {len(dataset)} logo examples")
            return dataset
            
        except Exception as e:
            logger.error(f"‚ùå Error preparing dataset: {e}")
            return []
    
    def setup_lora_config(self):
        """Setup LoRA configuration for logo training"""
        try:
            logger.info("üîß Setting up LoRA configuration...")
            
            self.lora_config = LoraConfig(
                r=16,  # Rank - higher = more parameters, better quality
                lora_alpha=32,  # Scaling factor
                target_modules=["to_k", "to_q", "to_v", "to_out.0"],  # Which layers to adapt
                lora_dropout=0.1,
                bias="none",
                task_type=TaskType.DIFFUSION,
            )
            
            logger.info("‚úÖ LoRA configuration ready")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error setting up LoRA: {e}")
            return False
    
    def load_base_model(self):
        """Load the base Stable Diffusion model"""
        try:
            logger.info("üîÑ Loading base model...")
            
            # Load pipeline
            pipe = StableDiffusionPipeline.from_pretrained(
                self.model_id,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                use_safetensors=True,
                variant="fp16" if self.device == "cuda" else None
            )
            
            # Optimize for training
            pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
            pipe = pipe.to(self.device)
            
            if self.device == "cuda":
                pipe.enable_attention_slicing()
                pipe.enable_vae_slicing()
            
            self.model = pipe.unet
            logger.info("‚úÖ Base model loaded")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error loading model: {e}")
            return False
    
    def apply_lora(self):
        """Apply LoRA to the model"""
        try:
            logger.info("üîß Applying LoRA to model...")
            
            self.model = get_peft_model(self.model, self.lora_config)
            logger.info("‚úÖ LoRA applied to model")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error applying LoRA: {e}")
            return False
    
    def train_lora(self, dataset, epochs=10, batch_size=1, learning_rate=1e-4):
        """Train the LoRA adapter"""
        try:
            logger.info(f"üéì Starting LoRA training for {epochs} epochs...")
            
            # Setup training
            optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)
            
            # Training loop
            for epoch in range(epochs):
                logger.info(f"üìö Epoch {epoch + 1}/{epochs}")
                
                for i, example in enumerate(dataset):
                    # Prepare inputs
                    image = example["image"]
                    prompt = example["prompt"]
                    
                    # Training step (simplified - real training is more complex)
                    optimizer.zero_grad()
                    
                    # Forward pass
                    # (This is simplified - real training involves noise prediction)
                    loss = self.compute_loss(image, prompt)
                    
                    # Backward pass
                    loss.backward()
                    optimizer.step()
                    
                    if i % 10 == 0:
                        logger.info(f"  Step {i}: Loss = {loss.item():.4f}")
                
                logger.info(f"‚úÖ Epoch {epoch + 1} completed")
            
            logger.info("üéâ LoRA training completed!")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error during training: {e}")
            return False
    
    def compute_loss(self, image, prompt):
        """Compute training loss (simplified)"""
        # This is a simplified loss computation
        # Real training involves noise prediction and denoising
        return torch.tensor(0.1, requires_grad=True)
    
    def save_lora(self, output_dir):
        """Save the trained LoRA adapter"""
        try:
            logger.info(f"üíæ Saving LoRA adapter to {output_dir}...")
            
            os.makedirs(output_dir, exist_ok=True)
            self.model.save_pretrained(output_dir)
            
            # Save configuration
            config = {
                "model_id": self.model_id,
                "lora_config": self.lora_config.to_dict(),
                "device": self.device,
                "trained_for": "logo_generation"
            }
            
            with open(os.path.join(output_dir, "config.json"), 'w') as f:
                json.dump(config, f, indent=2)
            
            logger.info("‚úÖ LoRA adapter saved")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error saving LoRA: {e}")
            return False

def create_sample_dataset():
    """Create a sample dataset for training"""
    try:
        logger.info("üìù Creating sample dataset...")
        
        # Create directories
        os.makedirs("logo_dataset/images", exist_ok=True)
        
        # Sample prompts for different logo types
        sample_prompts = {
            "tech_logo_1.png": {
                "prompt": "modern tech startup logo, geometric shapes, blue and white, professional",
                "style": "modern",
                "industry": "tech",
                "color": "blue"
            },
            "health_logo_1.png": {
                "prompt": "healthcare company logo, medical cross, green and white, trustworthy",
                "style": "professional",
                "industry": "healthcare",
                "color": "green"
            },
            "finance_logo_1.png": {
                "prompt": "financial services logo, shield symbol, navy blue, reliable",
                "style": "corporate",
                "industry": "finance",
                "color": "navy"
            },
            "creative_logo_1.png": {
                "prompt": "creative agency logo, artistic design, colorful, innovative",
                "style": "creative",
                "industry": "marketing",
                "color": "vibrant"
            }
        }
        
        # Save prompts
        with open("logo_dataset/prompts.json", 'w') as f:
            json.dump(sample_prompts, f, indent=2)
        
        logger.info("‚úÖ Sample dataset created")
        logger.info("üìÅ Add your logo images to 'logo_dataset/images/' directory")
        logger.info("üìù Edit 'logo_dataset/prompts.json' with your prompts")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Error creating dataset: {e}")
        return False

def main():
    """Main training function"""
    try:
        logger.info("üöÄ Starting Logo LoRA Training...")
        
        # Create sample dataset
        create_sample_dataset()
        
        # Initialize trainer
        trainer = LogoLoRATrainer()
        
        # Setup LoRA
        if not trainer.setup_lora_config():
            return False
        
        # Load base model
        if not trainer.load_base_model():
            return False
        
        # Apply LoRA
        if not trainer.apply_lora():
            return False
        
        # Load dataset
        dataset = trainer.prepare_logo_dataset("logo_dataset/images", "logo_dataset/prompts.json")
        if not dataset:
            logger.error("‚ùå No dataset found. Please add logo images and prompts.")
            return False
        
        # Train LoRA
        if not trainer.train_lora(dataset, epochs=5):
            return False
        
        # Save LoRA
        if not trainer.save_lora("trained_logo_lora"):
            return False
        
        logger.info("üéâ Logo LoRA training completed successfully!")
        logger.info("üìÅ Trained LoRA saved to 'trained_logo_lora/' directory")
        logger.info("üîß You can now use this LoRA with your logo generation!")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Training failed: {e}")
        return False

if __name__ == "__main__":
    main()


